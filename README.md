# evaluations
A unified toolkit for evaluating ML models, human annotators, and LLM judges â€” with metrics, uncertainty estimation, importance sampling, and reproducible evaluation protocols.
